# Scripts

Miscellaneous support scripts for HRM and other backend Aselo systems

## generate-conversation-json

This script will take the contents of plain text formatted books bulk downloaded from www.gutenberg.org and slice their contents up into fake transcripts of 'conversations' taking place between a helpline caller and a counsellor

In order to download the source data, run the following command in the root of the repo (not the /scripts directory) (in WSL if you are using Windows):

```
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en"
```

This will just run and run, you probably don't want to hoover up the entire catalog, so quit the command when you think you have enough base material to generate transcripts from

### Running the script

From `/scripts`

````shell
npm run build-then-generate-conversation-json <number-of-conversations>
````

Where <number-of-conversations> is the amount of conversation files you want to generate.

There are other options you can tweak, but they are not exposed as arguments currently, they are defined as consts in the `./src/generate-conversation-json.ts` file.

By default, the script expects the source books as zipped plain text files in a folder called `aleph.guteburg.org`, which is where they should be if you followed the instructions for downloading them above. By default the output JSON files will be written into `/transcripts-poc/convo-json`

If you generate more conversations than you have source text for, it will loop around and start reusing the same text again until the requested number of conversation files have been generated.

## import-conversation-postgres

This script will take JSON conversations generated by the generate-conversation-json script and write it to a local database with the schema defined in `/transcripts-poc/postgresql/hrm_transcripts_db.sql`

You need to have built & be running the docker compose file `/transcripts-poc/postgresql/docker-compose-persistent.yml` so you have a DB running that you can write to.

### Running the script

From `/scripts`

````shell
npm run build-then-import-conversation-postgres <number-of-conversations>
````

Where <number-of-conversations> is the amount of conversation files you want to upload.

There are other options you can tweak, but they are not exposed as arguments currently, they are defined as consts in the `./src/generate-conversation-json.ts` file.

By default, the input JSON files will be read from `/transcripts-poc/convo-json`

You cannot generate more conversations than you have source text for, it will stop processing when you either hit your specified number of conversations or run out of files.

## import-conversation-cloudsearch

This script will take JSON conversations generated by the generate-conversation-json script and upload it into a cloudsearch domain

It is currently hardcoded to point at the `transcript-poc` search domain on the Aselo account. You need AWS credentials set up on the standard environment variables for the script to work.

### Running the script

From `/scripts`

````shell
npm run build-then-import-conversation-cloudsearch <number-of-conversations>
````

Where <number-of-conversations> is the amount of conversation files you want to upload.

There are other options you can tweak, but they are not exposed as arguments currently, they are defined as consts in the `./src/generate-conversation-json.ts` file.

By default, the input JSON files will be read from `/transcripts-poc/convo-json`

You cannot generate more conversations than you have source text for, it will stop processing when you either hit your specified number of conversations or run out of files.

The script is configured to batch up conversations into JSON blobs as close to the limit of 5MB limit as it can, and upload them at a frequency of no more than 1 batch every 10 seconds, as per the recommendations in the AWS docs.